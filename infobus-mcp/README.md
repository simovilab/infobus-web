# Proyecto MCP con Mistral: Interfaz para hacer preguntas

Este documento detalla el proceso para configurar y ejecutar un sistema basado en el **Model Context Protocol (MCP)** que utiliza el modelo **Mistral** localmente mediante **Ollama**. La configuraci√≥n permite hacer preguntas a un modelo de IA y obtener respuestas a trav√©s de una interfaz cliente en Python.

# Gu√≠a de Implementaci√≥n del Proyecto MCP con Mistral

Este proyecto configura un servidor MCP usando el modelo Mistral a trav√©s de Ollama y un cliente para enviar preguntas y recibir respuestas.

## Requerimientos

Antes de ejecutar el proyecto, aseg√∫rate de tener las siguientes dependencias instaladas en tu entorno:

- Python 3.12 o superior
- Las siguientes librer√≠as de Python:

```txt
mcp[cli]
httpx
uvicorn
fastmcp
```

## Instalaci√≥n de Dependencias

```bash
python3 -m venv .venv
source .venv/bin/activate  # en Linux/Mac
```

1. Instala las dependencias con el siguiente comando:

```bash
pip install -r requirements.txt
```

## Comandos para Ejecutar el Proyecto

El proyecto consta de dos componentes principales: el servidor y el cliente. Debes correr ambos componentes en terminales separadas.

### 1. Terminal 1: Ejecutar el servidor

1. Entra al entorno virtual si no lo has hecho:

```bash
source .venv/bin/activate
```

2. Ejecuta el servidor con el siguiente comando:

```bash
uv run server/ollama_server.py
```

### 2. Terminal 2: Ejecutar el cliente

1. Entra al entorno virtual en una nueva terminal:

```bash
source .venv/bin/activate
```

2. Ejecuta el cliente con el siguiente comando:

```bash
python client/client.py
```

### 3. Realizar una Pregunta

1. En la terminal del cliente, puedes empezar a hacer preguntas. Ejemplo:

```bash
T√∫: ¬øQu√© es la inteligencia artificial?
```

2. El cliente enviar√° la solicitud al servidor y mostrar√° la respuesta del modelo.

## Soluci√≥n de Problemas

### Si tienes problemas con el servidor:

1. Verifica que el servidor est√° corriendo en el puerto 11434:

```bash
sudo ss -tuln | grep 11434
```

Si no est√° corriendo, aseg√∫rate de ejecutar el servidor con:

```bash
uv run server/ollama_server.py
```

### 3. Si el cliente no responde

Si el cliente no responde correctamente o muestra errores, sigue los pasos de soluci√≥n de problemas a continuaci√≥n:

#### Paso 1: Verifica que el servidor est√© en funcionamiento

Aseg√∫rate de que el servidor est√° corriendo correctamente en el puerto 11434. Para hacerlo, en la terminal donde ejecutaste el servidor, deber√≠as ver algo como:

```bash
[GIN] 2025/06/19 - 11:50:53 | 200 | 48.320876418s |       127.0.0.1 | POST     "/api/generate"
```

Si el servidor no est√° en ejecuci√≥n, puedes iniciar el servidor con el siguiente comando:

```bash
uv run server/ollama_server.py
```

Si despu√©s de ejecutar el servidor no ves la l√≠nea que indica que est√° escuchando en el puerto 11434, puedes verificar si est√° en uso con el siguiente comando:

```bash
sudo ss -tuln | grep 11434
```

Si el puerto no est√° siendo usado, aseg√∫rate de que el servidor est√© correctamente ejecut√°ndose y que no haya errores de configuraci√≥n.

#### Paso 2: Verifica que Ollama est√° corriendo

Es crucial que Ollama est√© corriendo en segundo plano para que el servidor pueda acceder al modelo Mistral. Si Ollama no est√° corriendo, el servidor no podr√° generar respuestas.

1. Para verificar si Ollama est√° en funcionamiento, ejecuta:

```bash
ps aux | grep ollama
```

Si no ves un proceso de `ollama serve` en la salida, intenta iniciar Ollama con:

```bash
ollama serve &
```

Esto deber√≠a iniciar Ollama en segundo plano, y podr√°s ver si se est√° ejecutando correctamente en la terminal.

#### Paso 3: Revisa el tiempo de espera (timeout)

Es posible que el cliente est√© esperando demasiado tiempo para obtener una respuesta del servidor y se quede "colgado". Si este es el caso, podr√≠as ver el siguiente mensaje en la terminal del cliente:

```bash
Error procesando la respuesta: timed out
```

Para evitar que el cliente se quede esperando indefinidamente, puedes aumentar el tiempo de espera en el cliente. Si el tiempo de espera sigue siendo muy corto, ajusta el par√°metro de `timeout` en la funci√≥n `enviar_pregunta` en el archivo `client.py`:

```python
response = httpx.post("http://localhost:11434", json=payload, timeout=60.0)  # Aumento el timeout a 60 segundos
```

#### Paso 4: Revisa las respuestas de la API

Si despu√©s de haber aumentado el tiempo de espera el cliente sigue sin responder, verifica la respuesta que est√° enviando el servidor. Para ello, imprime el cuerpo de la respuesta en el archivo `ollama_server.py` para asegurarte de que est√° recibiendo la respuesta del modelo correctamente. Debes ver algo como esto:

```bash
time=2025-06-19T11:50:53.884650722Z
response=" La Inteligencia Artificial (IA) es una rama de la ciencia..."
```

Si no est√°s recibiendo una respuesta v√°lida, aseg√∫rate de que el modelo Mistral est√© correctamente cargado y que Ollama lo est√© utilizando. Si el modelo no est√° cargado correctamente, podr√≠as intentar descargarlo de nuevo con el comando:

```bash
ollama pull mistral
```

#### Paso 5: Comprobaci√≥n de errores de conexi√≥n

Si todo parece estar bien, pero el cliente sigue sin recibir respuestas, podr√≠a haber problemas con la conexi√≥n entre el cliente y el servidor. Verifica los logs de la terminal donde corre el servidor para identificar si se est√° cerrando la conexi√≥n o si hay errores de red. Si es necesario, reinicia tanto el servidor como el cliente y prueba de nuevo.

#### Paso 6: Revisa el c√≥digo del cliente

Si todo lo anterior est√° correcto, pero a√∫n el cliente no responde, revisa que el c√≥digo del cliente est√© enviando correctamente las solicitudes. Aseg√∫rate de que el payload que se est√° enviando tenga el formato correcto. Debes ver algo similar a esto al enviar la solicitud:

```bash
üîç Enviando la siguiente solicitud al servidor: {
  "type": "tool",
  "tool": "preguntar",
  "args": {
    "prompt": "¬øQu√© es la inteligencia artificial?"
  }
}
```

Este mensaje indica que el cliente est√° enviando correctamente la solicitud al servidor, por lo que si no recibes respuesta, probablemente se deba a un error en la comunicaci√≥n entre el cliente y el servidor.

#### Paso 7: Reiniciar los servicios

Si despu√©s de seguir estos pasos a√∫n no funciona, reinicia tanto el servidor como Ollama y el cliente para asegurarte de que no haya procesos atascados.

```bash
sudo systemctl restart ollama
uv run server/ollama_server.py
python client/client.py
```

#### Paso 8: Verificar los logs

Si el error persiste, revisa los logs de ambos, el servidor y el cliente, para identificar cualquier mensaje de error adicional que pueda ayudarte a diagnosticar el problema.

---

## Descripci√≥n de Archivos

- `server/ollama_server.py`: El servidor que utiliza Ollama y Mistral para generar respuestas.
- `client/client.py`: El cliente que permite enviar preguntas y recibir respuestas desde el servidor.
- `requirements.txt`: Lista de dependencias necesarias para ejecutar el proyecto.

## Notas

- El proyecto utiliza el modelo Mistral que debe estar descargado previamente.
- En caso de que necesites cambiar el modelo o la configuraci√≥n, edita el archivo `server/ollama_server.py`.
